# LiteLLM Proxy Configuration Example
# Full documentation: https://docs.litellm.ai/docs/proxy/config_settings

model_list:  # List of model deployments https://docs.litellm.ai/docs/proxy/configs#model-list

  # Multiple deployments for the same model_name enables load balancing and failover
  # Weight: Higher weight = higher probability of being selected
  # Order: Lower order = higher priority in routing

  - model_name: claude-opus-4-5  # Alias used when calling the API https://docs.litellm.ai/docs/proxy/configs#model_name
    litellm_params:  # Parameters passed directly to litellm https://docs.litellm.ai/docs/proxy/configs#litellm_params
      model: anthropic/claude-opus-4-5-20251101  # Actual provider/model identifier https://docs.litellm.ai/docs/proxy/configs#model
      api_base: os.environ/CLIPROXYAPI_BASE_URL  # Custom API base URL from env var https://docs.litellm.ai/docs/proxy/configs#api_base
      api_key: os.environ/CLIPROXYAPI_KEY  # API key from env var https://docs.litellm.ai/docs/proxy/configs#api_key
      weight: 2  # Load balancing weight (higher = more traffic) https://docs.litellm.ai/docs/proxy/configs#weight
      order: 2  # Priority order (lower = higher priority) https://docs.litellm.ai/docs/proxy/configs#order

  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_base: os.environ/CLIPROXYAPI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
      weight: 2
      order: 2

  - model_name: claude-opus-4-5
    litellm_params:
      model: anthropic/gemini-claude-opus-4-5-thinking
      api_base: os.environ/CLIPROXYAPI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
      weight: 7
      order: 1

  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/gemini-claude-sonnet-4-5
      api_base: os.environ/CLIPROXYAPI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
      weight: 7
      order: 1

  - model_name: GLM-4.7
    litellm_params:
      model: zai/glm-4.7
      api_base: os.environ/ZAI_API_BASE
      api_key: os.environ/ZAI_API_KEY
      tpm: 150000  # Tokens per minute rate limit https://docs.litellm.ai/docs/proxy/configs#tpm

  - model_name: GLM-4.6
    litellm_params:
      model: zai/glm-4.6
      api_base: os.environ/ZAI_API_BASE
      api_key: os.environ/ZAI_API_KEY
      tpm: 150000

  - model_name: GLM-4.5-Air
    litellm_params:
      model: zai/glm-4.5-air
      api_base: os.environ/ZAI_API_BASE
      api_key: os.environ/ZAI_API_KEY
      tpm: 150000

  - model_name: minimax/MiniMax-M2.1
    litellm_params:
      model: minimax/MiniMax-M2.1
      api_base: os.environ/MINIMAX_API_BASE
      api_key: os.environ/MINIMAX_API_KEY
      tpm: 150000

  - model_name: minimax/MiniMax-M2.1-lightning
    litellm_params:
      model: minimax/MiniMax-M2.1-lightning
      api_base: os.environ/MINIMAX_API_BASE
      api_key: os.environ/MINIMAX_API_KEY
      tpm: 150000

# Global settings for all LLM calls https://docs.litellm.ai/docs/proxy/config_settings#litellm_settings---reference
litellm_settings:
  drop_params: True  # Drop unsupported params instead of failing https://docs.litellm.ai/docs/proxy/config_settings#drop_params
  success_callback: ["langfuse", "prometheus"]  # Callbacks on successful requests https://docs.litellm.ai/docs/proxy/logging
  failure_callback: ["prometheus", "langfuse"]  # Callbacks on failed requests
  langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY
  langfuse_host: os.environ/LANGFUSE_HOST
  check_provider_endpoint: true  # Validate provider endpoint before calls https://docs.litellm.ai/docs/proxy/config_settings
  turn_off_message_logging: False  # Log metadata only, not full messages https://docs.litellm.ai/docs/proxy/config_settings#turn_off_message_logging
  redact_user_api_key_info: true  # Hide sensitive key info in logs https://docs.litellm.ai/docs/proxy/config_settings#redact_user_api_key_info
  cache: False  # Enable/disable response caching https://docs.litellm.ai/docs/proxy/caching
  enable_caching_on_provider_specific_optional_params: True  # Cache provider-specific params https://docs.litellm.ai/docs/proxy/caching
  cache_params:  # Cache configuration https://docs.litellm.ai/docs/proxy/caching#supported-cache_params-on-proxy-configyaml
    type: redis  # Cache type: local, redis, s3, gcs https://docs.litellm.ai/docs/proxy/caching
    host: os.environ/REDIS_HOST  # Redis host from env var
    port: os.environ/REDIS_PORT  # Redis port from env var
    password: os.environ/REDIS_PASSWORD  # Redis password from env var
    ttl: 3600  # Default cache TTL in seconds (1 hour)

# Router configuration for load balancing and failover https://docs.litellm.ai/docs/proxy/config_settings#router_settings---reference
router_settings:
  routing_strategy: simple-shuffle # Literal["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle" - RECOMMENDED for best performance
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_checks: true  # Check context window before calls https://docs.litellm.ai/docs/proxy/reliability
  allowed_fails: 3  # Max failures before cooldown https://docs.litellm.ai/docs/proxy/config_settings#allowed_fails
  cooldown_time: 30  # Cooldown duration in seconds https://docs.litellm.ai/docs/proxy/config_settings#cooldown_time
  timeout: 300  # Default request timeout in seconds https://docs.litellm.ai/docs/proxy/config_settings#timeout
  stream_timeout: 180  # Streaming request timeout in seconds https://docs.litellm.ai/docs/proxy/config_settings#stream_timeout

# General proxy configuration https://docs.litellm.ai/docs/proxy/config_settings#general_settings---reference
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY  # Required for UI and Virtual Keys
  database_url: os.environ/DATABASE_URL  # Required for Virtual Keys
  ui_access_mode: admin_only  # Enable Admin UI for admins
  alerting: ["slack"]  # Alert channels for failures/errors https://docs.litellm.ai/docs/proxy/alerting
  store_model_in_db: true  # Persist model config in database https://docs.litellm.ai/docs/proxy/config_settings#store_model_in_db
  store_prompts_in_spend_logs: true  # Save prompts/responses in spend logs https://docs.litellm.ai/docs/proxy/config_settings#store_prompts_in_spend_logs
  maximum_spend_logs_retention_period: 3d  # Retain logs for 3 days only
  maximum_spend_logs_retention_interval: 1h  # Run cleanup task every hour
