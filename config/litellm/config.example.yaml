# LiteLLM Proxy Configuration Example
# Full documentation: https://docs.litellm.ai/docs/proxy/config_settings

# Centralized Credential Management
# Define credentials once and reuse across models for easier secret rotation
credential_list:
  - credential_name: cliproxyapi_credential
    credential_values:
      api_base: os.environ/CLIPROXYAPI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
    credential_info:
      description: "CLIProxyAPI credentials (Base)"

  - credential_name: cliproxyapi_credential_openai
    credential_values:
      api_base: os.environ/CLIPROXYAPI_OPENAI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
    credential_info:
      description: "CLIProxyAPI credentials (OpenAI-compatible)"

  - credential_name: cliproxyapi_credential_gemini
    credential_values:
      api_base: os.environ/CLIPROXYAPI_GEMINI_BASE_URL
      api_key: os.environ/CLIPROXYAPI_KEY
    credential_info:
      description: "CLIProxyAPI credentials (Gemini-compatible)"

  - credential_name: zai_coding_credential
    credential_values:
      api_base: https://api.z.ai/api/coding/paas/v4
      api_key: os.environ/ZAI_API_KEY
    credential_info:
      description: "Z.AI Coding Plan credentials (OpenAI-compatible)"

  - credential_name: zai_anthropic_credential
    credential_values:
      api_base: https://api.z.ai/api/anthropic
      api_key: os.environ/ZAI_API_KEY
    credential_info:
      description: "Z.AI Anthropic Format credentials"

model_list:
  # --------------------------------------------------
  # Antigravity models (CLIProxyAPI) - Priority order: 1
  # --------------------------------------------------
  - model_name: claude-sonnet-4-5
    litellm_params:
      model: anthropic/claude-sonnet-4-5-thinking
      litellm_credential_name: cliproxyapi_credential
      order: 1

  - model_name: claude-opus-4-5
    litellm_params:
      model: anthropic/claude-opus-4-5-thinking
      litellm_credential_name: cliproxyapi_credential
      order: 1

  - model_name: gemini-3-flash-preview
    litellm_params:
      model: gemini/gemini-3-flash-preview
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  - model_name: gemini-3-pro-preview
    litellm_params:
      model: gemini/gemini-3-pro-preview
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  - model_name: gemini-3-pro-image-preview
    litellm_params:
      model: gemini/gemini-3-pro-image-preview
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: gemini/gemini-2.5-flash-lite
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  # --------------------------------------------------
  # Google/Gemini-CLI models (CLIProxyAPI) - Fallback order: 2
  # --------------------------------------------------
  # Overlapping models - Gemini-CLI fallback (order: 2)
  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 1

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/google-gemini-2.5-flash
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 2

  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: gemini/google-gemini-2.5-flash-lite
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 2

  - model_name: gemini-3-flash-preview
    litellm_params:
      model: gemini/google-gemini-3-flash-preview
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 2

  - model_name: gemini-3-pro-preview
    litellm_params:
      model: gemini/google-gemini-3-pro-preview
      litellm_credential_name: cliproxyapi_credential_gemini
      order: 2

  # --------------------------------------------------
  # Z.AI Coding Plan - Optimized for Coding (OpenAI-compatible)
  # --------------------------------------------------
  - model_name: glm-4.7
    litellm_params:
      model: zai/glm-4.7
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.6
    litellm_params:
      model: zai/glm-4.6
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.6v
    litellm_params:
      model: zai/glm-4.6v
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.5
    litellm_params:
      model: zai/glm-4.5
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.5-flash
    litellm_params:
      model: zai/glm-4.5-flash
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.5-air
    litellm_params:
      model: zai/glm-4.5-air
      litellm_credential_name: zai_coding_credential
      order: 2

  - model_name: glm-4.5-airx
    litellm_params:
      model: zai/glm-4.5-airx
      litellm_credential_name: zai_coding_credential
      order: 2

  # --------------------------------------------------
  # Z.AI Anthropic Format - Anthropic-compatible Endpoint (same GLM models)
  # --------------------------------------------------
  - model_name: glm-4.7
    litellm_params:
      model: anthropic/glm-4.7
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 200000
      max_output_tokens: 128000
      input_cost_per_token: 6e-7
      output_cost_per_token: 0.0000022
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.6
    litellm_params:
      model: anthropic/glm-4.6
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 200000
      max_output_tokens: 128000
      input_cost_per_token: 6e-7
      output_cost_per_token: 0.0000022
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.6v
    litellm_params:
      model: anthropic/glm-4.6v
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 128000
      max_output_tokens: 128000
      input_cost_per_token: 3e-7
      output_cost_per_token: 9e-7
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: true
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.5
    litellm_params:
      model: anthropic/glm-4.5
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 128000
      max_output_tokens: 128000
      input_cost_per_token: 6e-7
      output_cost_per_token: 0.0000022
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.5-flash
    litellm_params:
      model: anthropic/glm-4.5-flash
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 128000
      max_output_tokens: 128000
      input_cost_per_token: 0
      output_cost_per_token: 0
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.5-air
    litellm_params:
      model: anthropic/glm-4.5-air
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 128000
      max_output_tokens: 128000
      input_cost_per_token: 2e-7
      output_cost_per_token: 0.0000011
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

  - model_name: glm-4.5-airx
    litellm_params:
      model: anthropic/glm-4.5-airx
      litellm_credential_name: zai_anthropic_credential
      order: 1
      use_in_pass_through: true
      max_input_tokens: 128000
      max_output_tokens: 128000
      input_cost_per_token: 0.0000011
      output_cost_per_token: 0.0000045
      supports_function_calling: true
      supports_tool_choice: true
      source: "https://docs.z.ai/guides/overview/pricing"
    model_info:
      mode: chat
      max_tokens: 128000
      max_input_tokens: 128000
      max_output_tokens: 128000
      supports_system_messages: true
      supports_prompt_caching: true
      supports_vision: false
      supports_audio_input: false
      supports_web_search: false
      supports_reasoning: true
      supports_function_calling: true
      supports_response_schema: true

# Search tools that AI can invoke and use.
# https://docs.litellm.ai/docs/search/
search_tools:
  - search_tool_name: firecrawl-search
    litellm_params:
      search_provider: firecrawl
      api_base: os.environ/FIRECRAWL_API_BASE
      api_key: os.environ/FIRECRAWL_API_KEY

# Global settings for all LLM calls https://docs.litellm.ai/docs/proxy/config_settings#litellm_settings---reference
litellm_settings:
  drop_params: True # Drop unsupported params instead of failing https://docs.litellm.ai/docs/proxy/config_settings#drop_params
  success_callback: ["langfuse", "prometheus"] # Callbacks on successful requests https://docs.litellm.ai/docs/proxy/logging
  failure_callback: ["prometheus", "langfuse"] # Callbacks on failed requests
  langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  langfuse_secret_key: os.environ/LANGFUSE_SECRET_KEY
  langfuse_host: os.environ/LANGFUSE_HOST
  check_provider_endpoint: true # Validate provider endpoint before calls https://docs.litellm.ai/docs/proxy/config_settings
  turn_off_message_logging: False # Log metadata only, not full messages https://docs.litellm.ai/docs/proxy/config_settings#turn_off_message_logging
  redact_user_api_key_info: true # Hide sensitive key info in logs https://docs.litellm.ai/docs/proxy/config_settings#redact_user_api_key_info
  cache: False # Enable/disable response caching https://docs.litellm.ai/docs/proxy/caching
  enable_caching_on_provider_specific_optional_params: True # Cache provider-specific params https://docs.litellm.ai/docs/proxy/caching
  cache_params: # Cache configuration https://docs.litellm.ai/docs/proxy/caching#supported-cache_params-on-proxy-configyaml
    type: redis # Cache type: local, redis, s3, gcs https://docs.litellm.ai/docs/proxy/caching
    host: os.environ/REDIS_HOST # Redis host from env var
    port: os.environ/REDIS_PORT # Redis port from env var
    password: os.environ/REDIS_PASSWORD # Redis password from env var
    ttl: 3600 # Default cache TTL in seconds (1 hour)

# Router configuration for load balancing and failover https://docs.litellm.ai/docs/proxy/config_settings#router_settings---reference
router_settings:
  routing_strategy: simple-shuffle # Literal["simple-shuffle", "least-busy", "usage-based-routing","latency-based-routing"], default="simple-shuffle" - RECOMMENDED for best performance
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD
  enable_pre_call_checks: true # Required for 'order' parameter to work https://docs.litellm.ai/docs/proxy/load_balancing#deployment-ordering-priority
  allowed_fails: 3 # Max failures before cooldown https://docs.litellm.ai/docs/proxy/config_settings#allowed_fails
  cooldown_time: 30 # Cooldown duration in seconds https://docs.litellm.ai/docs/proxy/config_settings#cooldown_time
  timeout: 300 # Default request timeout in seconds https://docs.litellm.ai/docs/proxy/config_settings#timeout
  stream_timeout: 180 # Streaming request timeout in seconds https://docs.litellm.ai/docs/proxy/config_settings#stream_timeout

# General proxy configuration https://docs.litellm.ai/docs/proxy/config_settings#general_settings---reference
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY # Required for UI and Virtual Keys
  database_url: os.environ/DATABASE_URL # Required for Virtual Keys
  ui_access_mode: admin_only # Enable Admin UI for admins
  alerting: ["slack"] # Alert channels for failures/errors https://docs.litellm.ai/docs/proxy/alerting
  store_model_in_db: true # Persist model config in database https://docs.litellm.ai/docs/proxy/config_settings#store_model_in_db
  store_prompts_in_spend_logs: true # Save prompts/responses in spend logs https://docs.litellm.ai/docs/proxy/config_settings#store_prompts_in_spend_logs
  maximum_spend_logs_retention_period: 3d # Retain logs for 3 days only
  maximum_spend_logs_retention_interval: 1h # Run cleanup task every hour
