version: "3.9"

services:
  cli-proxy-api:
    image: eceasy/cli-proxy-api:latest
    container_name: cli-proxy-api
    ports:
      - 8317:8317
    restart: always
    volumes:
      - ./config/cliproxyapi/config.yaml:/CLIProxyAPI/config.yaml
      - ./data/cliproxyapi/auth-dir:/root/.cli-proxy-api
    networks:
      - aiproxy-llm-connect
  litellm:
    image: docker.litellm.ai/berriai/litellm:main-stable
    container_name: litellm_proxy
    ports:
      - "4000:4000"
    restart: always
    volumes:
      - ./config/litellm/config.yaml:/app/config.yaml
    environment:
      STORE_MODEL_IN_DB: "True"
      CLIPROXYAPI_BASE_URL: "http://cli-proxy-api:8317"
    env_file:
      - .env
    depends_on:
      - db
    healthcheck:
      test:
        - CMD-SHELL
        - python3 -c "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness')"
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - aiproxy-llm-connect
  db:
    image: postgres:16
    container_name: litellm_db
    restart: always
    env_file:
      - .env
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -d litellm -U llmproxy" ]
      interval: 1s
      timeout: 5s
      retries: 10
    networks:
      - aiproxy-llm-connect

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    restart: always
    volumes:
      - ./data/prometheus:/prometheus
    #  - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    networks:
      - aiproxy-llm-connect

networks:
  aiproxy-llm-connect:


